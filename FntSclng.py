  
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
The objective of this code is to work with the data generated by RootPointer_New.py

Main Functions
    Qn_to_Qp : convert from microcanonical to canonical
    PlotAndFit + fitfunc : linear plot

Definitions in load
    try0 = np.loadtxt("AvgStats_L"+str(L0)+"_Samples50.dat")

    P_\inf = try0.T[0]
    \chi = np.divide(try0.T[2],try0.T[1], out = np.zeros_like(try0.T[2]), where = try0.T[1]!=0)
        \sum'_l l^2 n_l / \sum'_l l n_l

'Results'
    \gamma/\nu = 1.784(13)
    p_c = 0.594(3)
    \beta/\nu = 'very wide range' -1(1)

"""
import numpy as np
import scipy.stats as sstat
def Qn_to_Qp(p,Qn,N):
    " Gives Q(p) based on Qn "
    if type(Qn) is not np.ndarray : print("Qn MUST be a numpy array"); return
    
    factor = np.array([sstat.binom.pmf(n,N,p) for n in range(1,N)])
    Q_p = (factor*Qn).sum()
    return Q_p

def fitfunc(x,a,b):
    return a*x+b

from scipy.optimize import curve_fit
from matplotlib import pyplot as plt
def PlotAndFit(THE_x,THE_y,THE_yerr,LABEL_x,LABEL_y):
    " Plot and make a linearization for a given set of data "
    popt,pcov = curve_fit(fitfunc,THE_x,THE_y,sigma=THE_yerr)    
    xlin = np.linspace(THE_x[0],THE_x[len(THE_x)-1],100)
    
    plt.errorbar(THE_x,THE_y,yerr=THE_yerr,marker='.')
    plt.plot(xlin,fitfunc(xlin,*(popt)),'r-',label='%5.2f x + %5.2f'% tuple(popt))
    plt.xlabel(LABEL_x,fontsize=14)
    plt.ylabel(LABEL_y,fontsize=14)
    plt.legend(loc="upper left")
    plt.show()
    return popt[0],np.sqrt(pcov[0,0]),popt[1],np.sqrt(pcov[1,1])

import time

def MakeDataP(range_p,file,kind):
    """Range of p to be used. 
    Name of the file. 
    Column desired: 0 - P_\inf, 1 - \chi    
    
    ## It would be better if INSTEAD of saving in the end, I wrote line by line in the file.
    This way, I could avoid complete loss if code stops.
    
    """
    
    L_range = np.linspace(10,2000,100,dtype=int)
    L_range = L_range[:16] #Restriction to the input we have
    data_p = []
    
    for L0 in L_range:
        try0 = np.loadtxt("AvgStats_L"+str(L0)+"_Samples50.dat")
        if (kind==0) : 
            data_n = try0.T[0]
            data_p.append([Qn_to_Qp(p,data_n,L0*L0) for p in range_p])
        elif (kind==1) : 
            data_n = np.divide(try0.T[2],try0.T[1], out = np.zeros_like(try0.T[2]), where = try0.T[1]!=0)
            data_p.append([Qn_to_Qp(p,data_n,L0*L0) for p in range_p])
    
    Ls = "[ 10,  30,  50,  70,  90, 110, 130, 150, 170, 190, 211, 231, 251, 271, 291, 311]"
    tosave=np.array([list(range_p)]+data_p)
    np.savetxt(file+".dat",tosave,header="p; columns : L = "+Ls)
    return data_p

def MakeDataP_f(range_p,file,kind):
    """Range of p to be used. 
    Name of the file. 
    Column desired: 0 - P_\inf, 1 - \chi"""
    
    L_range = np.linspace(10,2000,100,dtype=int)
    L_range = L_range[:4] #Restriction to the input we have
    data_p = []
    
    for L0 in L_range:
        try0 = np.loadtxt("AvgStats_L"+str(L0)+"_Samples50.dat")
        if (kind==0) : 
            data_n = try0.T[0]
            data_p.append([Qn_to_Qp(p,data_n,L0*L0) for p in range_p])
        elif (kind==1) : 
            data_n = np.divide(try0.T[2],try0.T[1], out = np.zeros_like(try0.T[2]), where = try0.T[1]!=0)
            data_p.append([Qn_to_Qp(p,data_n,L0*L0) for p in range_p])
    
    Ls = "[ 10,  30,  50,  70,  90, 110, 130, 150, 170, 190, 211, 231, 251, 271, 291, 311]"
    tosave=np.array([list(range_p)]+data_p)
    np.savetxt(file+".dat",tosave,header="p; columns : L = "+Ls)
    return data_p


"""-------------------------- Peak vs L ---------------------------------------
\ln \chi (L,p) = \gamma/\nu \ln L + \ln \tilde \chi (\rho L^{1/\nu})
    gives \gamma/\nu in a \ln \chi x \ln L plot

onelog[0] \pm onelog[1] = \gamma/\nu

data with ACC=2; L_range[:16]
    \gamma/\nu = 1.71
"""
 
def GetPeak(L0,ACC):
    "Function to extract the peak of chi relative to p with certain accuracy"
    
    "Import data and get the useful column"
    try0 = np.loadtxt("AvgStats_L"+str(L0)+"_Samples50.dat")
#    data_n = try0.T[2]
    data_n = np.divide(try0.T[2],try0.T[1], out = np.zeros_like(try0.T[2]), where = try0.T[1]!=0)

    "GET peak of chi and value of the probability in the peak."
    range_p = np.linspace(0,1,10)
    data_p = [Qn_to_Qp(p,data_n,L0*L0) for p in range_p]
    V1_ind,V1_chi = max(enumerate(data_p),key=(lambda x : x[1]), default=-1)
    V1_p = range_p[V1_ind]
    V1_p_err = range_p[V1_ind+1]/2-range_p[V1_ind-1]/2
    V1_chi_err = max(-data_p[V1_ind+1]+data_p[V1_ind],data_p[V1_ind]-data_p[V1_ind-1])
    
    for ACC_LOOP in range(ACC):
        "Increasing the accuraccy of the former"
        range_p_ZOOM = np.linspace(V1_p-V1_p_err,V1_p+V1_p_err,10)
        data_p_ZOOM = [Qn_to_Qp(p,data_n,L0*L0) for p in range_p_ZOOM]
        V2_ind,V2_chi = max(enumerate(data_p_ZOOM),key=(lambda x : x[1]), default=-1)
        V2_p = range_p_ZOOM[V2_ind]
        V2_p_err = range_p_ZOOM[V2_ind+1]/2-range_p_ZOOM[V2_ind-1]/2
        V2_chi_err = max(-data_p_ZOOM[V2_ind+1]+data_p_ZOOM[V2_ind],data_p_ZOOM[V2_ind]-data_p_ZOOM[V2_ind-1])
        "Reset to prepare to the next loop"
        V1_p = V2_p
        V1_p_err = V2_p_err
        V1_chi = V2_chi
        V1_chi_err = V2_chi_err

    return V1_p, V1_chi, V1_p_err, V1_chi_err

def SavePeaks():
    "Peak dependence with L"
    L_range = np.linspace(10,2000,100,dtype=int)
    L_range = L_range[:16] #Restriction to the input we have
    peakdata = np.zeros((16,4),dtype=float)
    
    i=0
    for L0 in L_range[i:]:
        RunTime = time.time()
        peakdata[i] = GetPeak(L0,2)
        print(peakdata[i])
        i +=1
        
    np.savetxt("ChiMax.dat",peakdata)    
    return 

""" Fitting 
The fitting becomes better if we remove the initial grids. We must recall that 
    L_range[0]=10, L_range[1]=30
    
    \gamma/\nu = 
    [0:] 1.539(17)
    [1:] 1.712(16)
    [2:] 1.784(13)
"""
L_range = np.linspace(10,2000,100,dtype=int)
L_range = L_range[:16] #Restriction to the input we have
peakdata = np.loadtxt("ChiMax.dat")
onelog0 = PlotAndFit(np.log(L_range), np.log(peakdata.T[1]), peakdata.T[3]/peakdata.T[1],"$\ln L$", "$\ln \chi_{max}$")
onelog1 = PlotAndFit(np.log(L_range[1:]), np.log(peakdata.T[1][1:]), peakdata.T[3][1:]/peakdata.T[1][1:],"$\ln L$", "$\ln \chi_{max}$")
onelog2 = PlotAndFit(np.log(L_range[2:]), np.log(peakdata.T[1][2:]), peakdata.T[3][2:]/peakdata.T[1][2:],"$\ln L$", "$\ln \chi_{max}$")


"""-------------------------- P_\inf : p_c and \beta/\nu ----------------------
UNABLE to determine \beta/\nu with any accuraccy
p_c = 0.594(3)
""""

range_p = np.linspace(.5900,.6000,100)
MakeDataP(range_p,"Pinf_Zoom",0)

L_range = np.linspace(10,2000,100,dtype=int)
L_range = L_range[:16] #Restriction to the input we have
alldata = np.loadtxt("Pinf_ZOOM.dat")
range_p = alldata[0]
data_p = alldata[1:]

##Allows to visualize a 'fit' o \beta/\nu for a fixed p
#PlotAndFit(np.log(L_range),np.array(data_p).T[27],[.05]*16,"$\ln(L)$","$\ln(P_\inf)$")

##To visualize all curves in the zoomed region for a given \beta/\nu ratio
#betanu=.1
#range_p = np.linspace(.5900,.6000,100)
#for (data,L0) in zip(data_p[1:],L_range[1:]):
#    factor = np.float_power(L0,betanu)
#    plt.plot(range_p,factor*np.array(data))
#plt.grid(True)
#plt.xlim(.59,.60)
##plt.ylim(-.01,1.1)
#plt.xlabel("$p$",fontsize=14)
#plt.ylabel("$P_{\infty}$",fontsize=14)
#plt.title("betanu : "+str(betanu))
#plt.show()
    
"""
To obtain p_c and \beta/\nu we consider all crossing between curves of different L
\beta/\nu is chosen so that it minimizes the std error in p_c

There is a touch criterio to compute only "real" crossing.

We got that the best \beta/\nu = 0.073 which gives p_c = 0.594(3)
    in a certain way the error in \beta/\nu can assume any value that gives p_c in this range...

we can play a bit, but the result does not change a lot. What I find is that:
    - \beta/\nu is difficult to determine. The value of p_c is the above.

comment : in 'playing' I am referring to investigate the number of times one line 'touches' the other.
With the given precription I found 47 : i in range(5,16)
or... using all data max_touches = 103 (betanu = 0.0011, p_c = 0.594(3))
in this regime the best result is 0.592(2) at betanu = 0.1706
"""
low_mean = 0. 
low_std = 1.
low_betanu = 0
betanu_l =[]
max_touches = 0
for betanu in np.arange(0,.1,.0001):
    crosspoint = []
#    betanu = .067
    "find the point where the curve 'j' cross the curve 'i'." 
    touches = 0
    for i in range(16):
        factor0 = np.power(L_range[i],betanu)
        for j in range(i,16):
            #Takes care not to compute the same crossing twice
            factor = np.power(L_range[j],betanu)
            tominimize = abs(factor*np.array(data_p[j])-factor0*np.array(data_p[i]))
            ind,val = min(enumerate(tominimize),key=(lambda x : x[1]), default=-1)
            #check index where curves approximately meet each other
            if val<.001 : #touch criterio
                crosspoint.append(range_p[ind])
                #register probability of the index
                touches +=1
    
    if touches>40 : #to use ONLY 'betanu' that produre crossings in the interval
        mean = np.mean(np.array(crosspoint))
        std = np.std(np.array(crosspoint),dtype=np.float64)
        if touches > max_touches : 
            max_touches = touches
            touches_betanu=betanu
            touches_mean = mean
            touches_std = std
        #get the mean value of p_c and the standard deviation 
        if (std < low_std) : 
            low_std = std
            low_mean = mean
            low_betanu = betanu
        
#    if (mean>(0.591))*(mean<(0.597)):
#        betanu_l.append(betanu)


"""-------------------------- To Do : \nu--------------------------------------
Plot of L^{\gamma/\nu} \chi vs (p-pc)/pc


choose a arbitrary \tilde \chi which determines an array \rho
with this, make the plot

\ln \rho = -1/\nu \ln L + residues

compare with other arbitrary choices of \tilde \chi
"""

range_p = np.linspace(.1,.9,100)
data_p = MakeDataP(range_p,"Chi_p",1)

"import"
L_range = np.linspace(10,2000,100,dtype=int)
L_range = L_range[:16] #Restriction to the input we have
alldata = np.loadtxt("Chi_p.dat")
range_p = alldata[0]
data_p = alldata[1:]

"rescaling"
gammanu = 1.78 # \pm 0.02. O valor real seria 1.76 (qual a fonte dessa info???)
data_p_scale = data_p.copy()
for i in range(len(L_range)):
    factor = np.power(L_range[i],-gammanu)
    data_p_scale[i] = factor*np.array(data_p[i])

"plot"
for data in data_p_scale:
    plt.plot(range_p,data,'o-')    
plt.grid(True)
plt.xlim(.5,.65)
#plt.ylim(-.01,1.1)
plt.xlabel("$p$",fontsize=14)
plt.ylabel("$L^{-\\gamma/\\nu} \chi$",fontsize=14)
plt.show()

"""
parece possível tomar algo como invnu = 0.73 numa estimativa bruta
    \nu = 1/0.73 = 1.37
    o esperado era algo como 4/3 = 1.33
    
    
considero que o mais coerente seria traçar uma reta de \tilde \chi fixo e
 minimizar a distância entre a largura das curvas
 
um modo talvez mais inteligente seria realizar um fit gaussiano das curvas
 e então tratar esse fit

"""

for invnu in np.arange(.72,.74,.01):
    "second scaling"
#    invnu = .7
    pc = 0.594
    range_rho = (range_p-pc)/pc
    range_rho_scale = np.zeros_like(data_p_scale) 
    #range split and becomes differente for each curve
    for i in range(len(L_range)):
        factor = np.power(L_range[i],invnu)
        range_rho_scale[i] = factor*np.array(range_rho)
    
    
    "plot"
    for (xvalue,yvalue) in zip(range_rho_scale[1:],data_p_scale[1:]):
        plt.plot(xvalue,yvalue,'o-')
    plt.grid(True)
    plt.xlim(-2.5,2)
    plt.ylim(.1,.5)
    plt.xlabel("$(p-p_c)/p_c$",fontsize=14)
    plt.ylabel("$L^{-\\gamma/\\nu} \chi$",fontsize=14)
    plt.show()


"-------------------------- To Do : \nu----------------------------------------"
"Plot of L^{-\beta/\nu} P_\inf vs (p-pc)/pc"

range_p = np.linspace(.1,.9,100)
data_p = MakeDataP(range_p,"Pinf_p",0)
